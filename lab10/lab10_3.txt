Task 1: What does the confusion matrix show for this example?
The confusion matrix shows which digits/labels were confused with other digits/labels in this example. It plots
the true label with against the predicted label and shows what the true label matched with in the model.
Task 2: How does the TensorFlow network architecture differ from the Keras example given in class? Report any improvements you can make over the baseline testset accuracy for this task.
The TensorFlow network architecture has a few differences from the architecture we used in class. First, the optimizer between the two is different.
The Google TensorFlow network uses Adagrad while in class we used rmsprop. The error is measured using logloss as well instead of
categorical crossentropy(although it appears as if these are mostly the same from some googling?). In the Keras example the learning rate
is not specified (I assume the optimizer sets it automatically), but this is a fairly large difference from having to specify it
every time in Tensor flow. Steps per epoch is also not directly specified in the Keras example.
Finally, the activation function also does not seem to be specified in the tensorflow code.

Using learning_rate=0.06,steps=2000,batch_size=100,hidden_units=[512, 100] gave an improvement to .96 accuracy which is
an improvement over the baseline of .95.

Task 3: What differences can you see between the visualizations for 10 steps and 1000 steps?
Using 10 steps, the resulting visualizations are, for the most part, noise and do not have defined shapes in them.
With 1000 steps the visualizations have defined shapes and patterns in them. They do not seem like random noise.